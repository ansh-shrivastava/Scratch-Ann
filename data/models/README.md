Scratch Artificial Neural Network (MNIST)

A fully implemented feedforward Artificial Neural Network from scratch using NumPy, trained on the MNIST handwritten digit dataset using:

Backpropagation

Mini-batch Stochastic Gradient Descent (SGD)

Sigmoid activation function

No deep learning frameworks (TensorFlow / PyTorch) were used.

Architecture
Input Layer:      784 neurons (28x28 MNIST image)
Hidden Layer 1:   50 neurons
Hidden Layer 2:   30 neurons
Output Layer:     10 neurons (digit classes 0â€“9)


Model initialization:

net1 = network([784, 50, 30, 10])

 Mathematical Overview
Forward Propagation

For each layer:

ğ‘
=
ğœ
(
ğ‘Š
ğ‘
+
ğ‘
)
a=Ïƒ(Wa+b)

Where:

ğ‘Š
W = weights

ğ‘
b = biases

ğœ
Ïƒ = sigmoid activation

Backpropagation

Output layer error:

ğ›¿
=
(
ğ‘
âˆ’
ğ‘¦
)
â‹…
ğœ
â€²
(
ğ‘§
)
Î´=(aâˆ’y)â‹…Ïƒ
â€²
(z)

Hidden layer error:

ğ›¿
ğ‘™
=
(
ğ‘Š
ğ‘™
+
1
)
ğ‘‡
ğ›¿
ğ‘™
+
1
â‹…
ğœ
â€²
(
ğ‘§
ğ‘™
)
Î´
l
=(W
l+1
)
T
Î´
l+1
â‹…Ïƒ
â€²
(z
l
)

Weights update rule:

ğ‘Š
=
ğ‘Š
âˆ’
ğœ‚
1
ğ‘š
âˆ‡
ğ‘Š
W=Wâˆ’Î·
m
1
	â€‹

âˆ‡W

Where:

ğœ‚
Î· = learning rate

ğ‘š
m = mini-batch size

 Training Configuration
net1.sgd(training_data, 200, 10, 0.001, test_data=test_data)


Epochs: 200

Mini-batch size: 10

Learning rate: 0.001